{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ElEwyQZmImOH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "51cfbf8b-d417-49d5-b764-8fd0415294f6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Mounting Drive\\nfrom google.colab import drive\\ndrive.mount(\\'/content/drive\\')\\n\\n\\n#---------------------------------------------------------\\n# Necessary imports\\nimport glob\\nimport pandas as pd\\nimport urllib.request\\nimport imageio\\nimport os\\nimport numpy as np\\n\\nfrom urllib.request import urlopen\\n\\n\\n#---------------------------------------------------------\\n# Load Glove Function\\ndef loadGloveModel(gloveFile):\\n    print(\"Loading Glove Model\")\\n    f = open(gloveFile,\\'r\\',encoding=\"utf8\")\\n    model = {}\\n    for line in f:\\n        try:\\n            splitLine = line.split()\\n            word = splitLine[0]\\n            embedding = np.array([float(val) for val in splitLine[1:]])\\n            model[word] = embedding\\n        except:\\n            print(word)\\n    print(\"Done.\",len(model),\" words loaded!\")\\n    return model\\n\\n#---------------------------------------------------------    \\n\\nglove_embeddings = loadGloveModel(\"/content/drive/MyDrive/glove.6B.300d.txt\")\\n\\n#---------------------------------------------------------    \\n# Setting up the paths\\ntrain_data_path = \"/content/drive/MyDrive/flowers\"\\ntrain_images_path = \"/content/drive/MyDrive/flowers/jpg\"\\ntrain_captions_path = \"/content/drive/MyDrive/flowers/text_c10\"\\n\\n#---------------------------------------------------------    \\n# Imports\\nimport tensorflow as tf\\nfrom tensorflow.keras.layers import Input, Reshape, Dropout, Dense, Concatenate \\nfrom tensorflow.keras.layers import Flatten, BatchNormalization\\nfrom tensorflow.keras.layers import Activation, ZeroPadding2D\\nfrom tensorflow.keras.layers import LeakyReLU\\nfrom tensorflow.keras.layers import UpSampling2D, Conv2D, Conv2DTranspose\\nfrom tensorflow.keras.models import Sequential, Model, load_model\\nfrom tensorflow.keras.optimizers import Adam\\nfrom tensorflow.keras.utils import plot_model\\nfrom tensorflow.keras import initializers\\nfrom sklearn.metrics import mean_squared_error\\n\\nimport numpy as np\\nfrom PIL import Image\\nfrom tqdm import tqdm\\nimport os \\nimport time\\nimport matplotlib.pyplot as plt\\n#--------------------------------------------------------- \\n\\n\\n# Formatted time string\\ndef hms_string(sec_elapsed):\\n    h = int(sec_elapsed / (60 * 60))\\n    m = int((sec_elapsed % (60 * 60)) / 60)\\n    s = sec_elapsed % 60\\n    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\\n     \\n#---------------------------------------------------------    \\n# Generation resolution - Must be square \\n# Training data is also scaled to this.\\nGENERATE_RES = 2 # Generation resolution factor \\n# (1=32, 2=64, 3=96, 4=128, etc.)\\nGENERATE_SQUARE = 32 * GENERATE_RES # rows/cols (should be square)\\nIMAGE_CHANNELS = 3\\n\\n# Preview image \\nPREVIEW_ROWS = 4\\nPREVIEW_COLS = 7\\nPREVIEW_MARGIN = 16\\n\\n# Size vector to generate images from\\nSEED_SIZE = 100\\nEMBEDDING_SIZE = 300\\n\\n# Configuration\\nDATA_PATH = train_images_path\\nMODEL_PATH = \"/content/drive/Shareddrives/D4NLP Project/flowers data/flowers/model\"\\nEPOCHS = 50\\nBATCH_SIZE = 64\\nBUFFER_SIZE = 4000\\n\\nprint(f\"Will generate {GENERATE_SQUARE}px square images.\")\\n\\n#---------------------------------------------------------    \\n\\n# Image set has 8,188 images.  Can take over an hour \\n# for initial preprocessing.\\n# Because of this time needed, save a Numpy preprocessed file.\\n\\ntraining_binary_path = os.path.join(\"/content/drive/Shareddrives/D4NLP Project/flowers data/flowers/images/npy64\",\\n        f\\'training_data_{GENERATE_SQUARE}_{GENERATE_SQUARE}_\\')\\n\\nstart = time.time()\\nprint(\"Loading training images...\")\\n\\ntraining_data = []\\nflowers_path = sorted(os.listdir(DATA_PATH))\\n\\nfor filename in range(len(flowers_path)):\\n    path = os.path.join(DATA_PATH,flowers_path[filename])\\n    # print(path)\\n    try:\\n      image = Image.open(path).resize((GENERATE_SQUARE,\\n            GENERATE_SQUARE),Image.ANTIALIAS)\\n      channel = np.asarray(image).shape[2]\\n      if channel == 3:\\n        training_data.append(np.asarray(image))\\n    except KeyboardInterrupt:\\n        print(\"Keyboard Interrup by me...\")\\n        break\\n    except:\\n      pass\\n    if len(training_data) == 100:\\n      training_data = np.reshape(training_data,(-1,GENERATE_SQUARE,\\n                GENERATE_SQUARE,IMAGE_CHANNELS))\\n      training_data = training_data.astype(np.float32)\\n      #Normalizing the input\\n      training_data = training_data / 127.5 - 1.\\n\\n      print(\"Saving training image \" + str(100000 + filename) + \".npy\")\\n      np.save(training_binary_path + str(100000 + filename) + \".npy\",training_data)\\n      elapsed = time.time()-start\\n      print (f\\'Image preprocess time: {hms_string(elapsed)}\\')\\n      training_data = []\\nprint(\"Complete\")\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "\"\"\"\n",
        "# Mounting Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "#---------------------------------------------------------\n",
        "# Necessary imports\n",
        "import glob\n",
        "import pandas as pd\n",
        "import urllib.request\n",
        "import imageio\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "from urllib.request import urlopen\n",
        "\n",
        "\n",
        "#---------------------------------------------------------\n",
        "# Load Glove Function\n",
        "def loadGloveModel(gloveFile):\n",
        "    print(\"Loading Glove Model\")\n",
        "    f = open(gloveFile,'r',encoding=\"utf8\")\n",
        "    model = {}\n",
        "    for line in f:\n",
        "        try:\n",
        "            splitLine = line.split()\n",
        "            word = splitLine[0]\n",
        "            embedding = np.array([float(val) for val in splitLine[1:]])\n",
        "            model[word] = embedding\n",
        "        except:\n",
        "            print(word)\n",
        "    print(\"Done.\",len(model),\" words loaded!\")\n",
        "    return model\n",
        "\n",
        "#---------------------------------------------------------    \n",
        "\n",
        "glove_embeddings = loadGloveModel(\"/content/drive/MyDrive/glove.6B.300d.txt\")\n",
        "\n",
        "#---------------------------------------------------------    \n",
        "# Setting up the paths\n",
        "train_data_path = \"/content/drive/MyDrive/flowers\"\n",
        "train_images_path = \"/content/drive/MyDrive/flowers/jpg\"\n",
        "train_captions_path = \"/content/drive/MyDrive/flowers/text_c10\"\n",
        "\n",
        "#---------------------------------------------------------    \n",
        "# Imports\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Reshape, Dropout, Dense, Concatenate \n",
        "from tensorflow.keras.layers import Flatten, BatchNormalization\n",
        "from tensorflow.keras.layers import Activation, ZeroPadding2D\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.layers import UpSampling2D, Conv2D, Conv2DTranspose\n",
        "from tensorflow.keras.models import Sequential, Model, load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras import initializers\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import os \n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "#--------------------------------------------------------- \n",
        "\n",
        "\n",
        "# Formatted time string\n",
        "def hms_string(sec_elapsed):\n",
        "    h = int(sec_elapsed / (60 * 60))\n",
        "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
        "    s = sec_elapsed % 60\n",
        "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
        "     \n",
        "#---------------------------------------------------------    \n",
        "# Generation resolution - Must be square \n",
        "# Training data is also scaled to this.\n",
        "GENERATE_RES = 2 # Generation resolution factor \n",
        "# (1=32, 2=64, 3=96, 4=128, etc.)\n",
        "GENERATE_SQUARE = 32 * GENERATE_RES # rows/cols (should be square)\n",
        "IMAGE_CHANNELS = 3\n",
        "\n",
        "# Preview image \n",
        "PREVIEW_ROWS = 4\n",
        "PREVIEW_COLS = 7\n",
        "PREVIEW_MARGIN = 16\n",
        "\n",
        "# Size vector to generate images from\n",
        "SEED_SIZE = 100\n",
        "EMBEDDING_SIZE = 300\n",
        "\n",
        "# Configuration\n",
        "DATA_PATH = train_images_path\n",
        "MODEL_PATH = \"/content/drive/Shareddrives/D4NLP Project/flowers data/flowers/model\"\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 4000\n",
        "\n",
        "print(f\"Will generate {GENERATE_SQUARE}px square images.\")\n",
        "\n",
        "#---------------------------------------------------------    \n",
        "\n",
        "# Image set has 8,188 images.  Can take over an hour \n",
        "# for initial preprocessing.\n",
        "# Because of this time needed, save a Numpy preprocessed file.\n",
        "\n",
        "training_binary_path = os.path.join(\"/content/drive/Shareddrives/D4NLP Project/flowers data/flowers/images/npy64\",\n",
        "        f'training_data_{GENERATE_SQUARE}_{GENERATE_SQUARE}_')\n",
        "\n",
        "start = time.time()\n",
        "print(\"Loading training images...\")\n",
        "\n",
        "training_data = []\n",
        "flowers_path = sorted(os.listdir(DATA_PATH))\n",
        "\n",
        "for filename in range(len(flowers_path)):\n",
        "    path = os.path.join(DATA_PATH,flowers_path[filename])\n",
        "    # print(path)\n",
        "    try:\n",
        "      image = Image.open(path).resize((GENERATE_SQUARE,\n",
        "            GENERATE_SQUARE),Image.ANTIALIAS)\n",
        "      channel = np.asarray(image).shape[2]\n",
        "      if channel == 3:\n",
        "        training_data.append(np.asarray(image))\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"Keyboard Interrup by me...\")\n",
        "        break\n",
        "    except:\n",
        "      pass\n",
        "    if len(training_data) == 100:\n",
        "      training_data = np.reshape(training_data,(-1,GENERATE_SQUARE,\n",
        "                GENERATE_SQUARE,IMAGE_CHANNELS))\n",
        "      training_data = training_data.astype(np.float32)\n",
        "      #Normalizing the input\n",
        "      training_data = training_data / 127.5 - 1.\n",
        "\n",
        "      print(\"Saving training image \" + str(100000 + filename) + \".npy\")\n",
        "      np.save(training_binary_path + str(100000 + filename) + \".npy\",training_data)\n",
        "      elapsed = time.time()-start\n",
        "      print (f'Image preprocess time: {hms_string(elapsed)}')\n",
        "      training_data = []\n",
        "print(\"Complete\")\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U featuretools"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwHqLLJONoGz",
        "outputId": "4c76daab-bd87-4f79-bd9b-d83e2cd34ca4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: featuretools in /usr/local/lib/python3.8/dist-packages (1.19.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.8/dist-packages (from featuretools) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from featuretools) (1.7.3)\n",
            "Requirement already satisfied: cloudpickle>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from featuretools) (1.5.0)\n",
            "Requirement already satisfied: pandas>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from featuretools) (1.5.2)\n",
            "Requirement already satisfied: tqdm>=4.32.0 in /usr/local/lib/python3.8/dist-packages (from featuretools) (4.64.1)\n",
            "Requirement already satisfied: holidays>=0.13 in /usr/local/lib/python3.8/dist-packages (from featuretools) (0.17.2)\n",
            "Requirement already satisfied: psutil>=5.6.6 in /usr/local/lib/python3.8/dist-packages (from featuretools) (5.9.4)\n",
            "Requirement already satisfied: dask[dataframe]!=2022.10.1,>=2022.2.0 in /usr/local/lib/python3.8/dist-packages (from featuretools) (2022.2.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from featuretools) (21.3)\n",
            "Requirement already satisfied: woodwork[dask]>=0.18.0 in /usr/local/lib/python3.8/dist-packages (from featuretools) (0.21.1)\n",
            "Requirement already satisfied: distributed!=2022.10.1,>=2022.2.0 in /usr/local/lib/python3.8/dist-packages (from featuretools) (2022.2.1)\n",
            "Requirement already satisfied: partd>=0.3.10 in /usr/local/lib/python3.8/dist-packages (from dask[dataframe]!=2022.10.1,>=2022.2.0->featuretools) (1.3.0)\n",
            "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from dask[dataframe]!=2022.10.1,>=2022.2.0->featuretools) (2022.11.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.8/dist-packages (from dask[dataframe]!=2022.10.1,>=2022.2.0->featuretools) (6.0)\n",
            "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.8/dist-packages (from dask[dataframe]!=2022.10.1,>=2022.2.0->featuretools) (0.12.0)\n",
            "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.8/dist-packages (from distributed!=2022.10.1,>=2022.2.0->featuretools) (2.4.0)\n",
            "Requirement already satisfied: tornado>=6.0.3 in /usr/local/lib/python3.8/dist-packages (from distributed!=2022.10.1,>=2022.2.0->featuretools) (6.0.4)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from distributed!=2022.10.1,>=2022.2.0->featuretools) (1.7.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from distributed!=2022.10.1,>=2022.2.0->featuretools) (57.4.0)\n",
            "Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.8/dist-packages (from distributed!=2022.10.1,>=2022.2.0->featuretools) (7.1.2)\n",
            "Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.8/dist-packages (from distributed!=2022.10.1,>=2022.2.0->featuretools) (2.2.0)\n",
            "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from distributed!=2022.10.1,>=2022.2.0->featuretools) (1.0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from distributed!=2022.10.1,>=2022.2.0->featuretools) (2.11.3)\n",
            "Requirement already satisfied: convertdate>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from holidays>=0.13->featuretools) (2.4.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.8/dist-packages (from holidays>=0.13->featuretools) (2.8.2)\n",
            "Requirement already satisfied: hijri-converter in /usr/local/lib/python3.8/dist-packages (from holidays>=0.13->featuretools) (2.2.4)\n",
            "Requirement already satisfied: korean-lunar-calendar in /usr/local/lib/python3.8/dist-packages (from holidays>=0.13->featuretools) (0.3.1)\n",
            "Requirement already satisfied: pymeeus<=1,>=0.3.13 in /usr/local/lib/python3.8/dist-packages (from convertdate>=2.3.0->holidays>=0.13->featuretools) (0.5.12)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->featuretools) (3.0.9)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.4.0->featuretools) (2022.6)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.8/dist-packages (from partd>=0.3.10->dask[dataframe]!=2022.10.1,>=2022.2.0->featuretools) (1.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil->holidays>=0.13->featuretools) (1.15.0)\n",
            "Requirement already satisfied: importlib-resources>=5.10.0 in /usr/local/lib/python3.8/dist-packages (from woodwork[dask]>=0.18.0->featuretools) (5.10.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.8/dist-packages (from woodwork[dask]>=0.18.0->featuretools) (1.0.2)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=5.10.0->woodwork[dask]>=0.18.0->featuretools) (3.11.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.22->woodwork[dask]>=0.18.0->featuretools) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.22->woodwork[dask]>=0.18.0->featuretools) (3.1.0)\n",
            "Requirement already satisfied: heapdict in /usr/local/lib/python3.8/dist-packages (from zict>=0.1.3->distributed!=2022.10.1,>=2022.2.0->featuretools) (1.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->distributed!=2022.10.1,>=2022.2.0->featuretools) (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opendatasets --upgrade"
      ],
      "metadata": {
        "id": "j5J56PtFdXML",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43fba09a-2d15-461e-b73b-68c229bda424"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: opendatasets in /usr/local/lib/python3.8/dist-packages (0.1.22)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from opendatasets) (4.64.1)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.8/dist-packages (from opendatasets) (1.5.12)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from opendatasets) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from kaggle->opendatasets) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.8/dist-packages (from kaggle->opendatasets) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.8/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.8/dist-packages (from kaggle->opendatasets) (1.24.3)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.8/dist-packages (from kaggle->opendatasets) (7.0.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from kaggle->opendatasets) (2022.12.7)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.8/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->kaggle->opendatasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->kaggle->opendatasets) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "GPpt0A2ytLzJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "outputId": "b9c90e73-4b0d-4e69-a8ef-7655664f5fa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2ca596bd-0714-4ea7-8c71-8c9649c2c1a5\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2ca596bd-0714-4ea7-8c71-8c9649c2c1a5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle"
      ],
      "metadata": {
        "id": "EyRvSIEJtL2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "BcCIqGh-tL5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "BiLVRQr_tL7x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29c549dc-19fa-4c0c-cc03-e0234e12f74e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bi_skip.npz\t dictionary.txt\t\t   images1.pickle  sample_data\n",
            "bi_skip.npz.pkl  drive\t\t\t   images2.pickle  uni_skip.npz\n",
            "btable.npy\t flickr30k_images\t   images3.pickle  uni_skip.npz.pkl\n",
            "captions.pickle  flickr-image-dataset.zip  kaggle.json\t   utable.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d hsankesara/flickr-image-dataset"
      ],
      "metadata": {
        "id": "wzcNPHDLtL-q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8218cd59-7918-4bbc-da55-95057a9d6835"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "flickr-image-dataset.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "tRyuidugtMBi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ce25582-ea9b-4b41-d0e2-a86406ee3248"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bi_skip.npz\t dictionary.txt\t\t   images1.pickle  sample_data\n",
            "bi_skip.npz.pkl  drive\t\t\t   images2.pickle  uni_skip.npz\n",
            "btable.npy\t flickr30k_images\t   images3.pickle  uni_skip.npz.pkl\n",
            "captions.pickle  flickr-image-dataset.zip  kaggle.json\t   utable.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip flickr-image-dataset.zip"
      ],
      "metadata": {
        "id": "J3n4B_yYj9QO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d5a0016-aab7-4ffa-bdf3-1fb4e798c3b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  flickr-image-dataset.zip\n",
            "replace flickr30k_images/flickr30k_images/1000092795.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wftvg8Lgmme7",
        "outputId": "b9945e81-f0d2-41aa-f796-bd157bc22b61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bi_skip.npz\t dictionary.txt\t\t   images1.pickle  sample_data\n",
            "bi_skip.npz.pkl  drive\t\t\t   images2.pickle  uni_skip.npz\n",
            "btable.npy\t flickr30k_images\t   images3.pickle  uni_skip.npz.pkl\n",
            "captions.pickle  flickr-image-dataset.zip  kaggle.json\t   utable.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cd flickr30k_images/flickr30k_images"
      ],
      "metadata": {
        "id": "ZavVCg9Zmpn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cd -"
      ],
      "metadata": {
        "id": "4tRrx0wL21tD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pwd # /content/flickr30k_images/flickr30k_images"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "95tjfv5O8NY6",
        "outputId": "cfbcb522-3e12-4e97-834f-efb221b49761"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NRQczju2dgb",
        "outputId": "587b88cf-89c6-414c-dff8-ba6a6ff21f1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " drive\t\t    flickr-image-dataset.zip   kaggle.json\n",
            " flickr30k_images  'kaggle (1).json'\t       sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install theano"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsClJk5PZ_YZ",
        "outputId": "f0b47a14-4d93-4fec-99d9-62a89999f241"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: theano in /usr/local/lib/python3.8/dist-packages (1.0.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.8/dist-packages (from theano) (1.7.3)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from theano) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.8/dist-packages (from theano) (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Caption.py"
      ],
      "metadata": {
        "id": "z6EtLqbRfBYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from zipfile import ZipFile\n",
        "from pandas import read_csv\n",
        "from io import BytesIO\n",
        "import numpy as np\n",
        "from pickle import dump, load\n",
        "\n",
        "from PIL import Image\n",
        "from cv2 import resize\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import theano\n",
        "import theano.tensor as tensor\n",
        "\n",
        "import pickle as pkl\n",
        "import numpy\n",
        "import copy\n",
        "import nltk\n",
        "\n",
        "from collections import OrderedDict, defaultdict\n",
        "from scipy.linalg import norm\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "import time\n",
        "import datetime\n",
        "import logging\n",
        "import itertools\n",
        "\n",
        "import torch.optim as optim\n",
        "from torchvision.utils import save_image"
      ],
      "metadata": {
        "id": "EtuI7J6jtMHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "archive = ZipFile(file=\"/content/flickr-image-dataset.zip\", mode=\"r\")\n",
        "# archive.printdir() # prints all the contents of the zip file"
      ],
      "metadata": {
        "id": "-JkuAduEjhgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = read_csv(\"/content/flickr30k_images/flickr30k_images/results.csv\", sep=\"|\")\n",
        "df = df.rename(columns=lambda x: x.strip())"
      ],
      "metadata": {
        "id": "Ih4MBnUvtMMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "LqiWYWiEhh4V",
        "outputId": "357c81ad-720e-4d77-cef0-9cbb53f55600"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            image_name comment_number  \\\n",
              "0       1000092795.jpg              0   \n",
              "1       1000092795.jpg              1   \n",
              "2       1000092795.jpg              2   \n",
              "3       1000092795.jpg              3   \n",
              "4       1000092795.jpg              4   \n",
              "...                ...            ...   \n",
              "158910   998845445.jpg              0   \n",
              "158911   998845445.jpg              1   \n",
              "158912   998845445.jpg              2   \n",
              "158913   998845445.jpg              3   \n",
              "158914   998845445.jpg              4   \n",
              "\n",
              "                                                  comment  \n",
              "0        Two young guys with shaggy hair look at their...  \n",
              "1        Two young , White males are outside near many...  \n",
              "2        Two men in green shirts are standing in a yard .  \n",
              "3            A man in a blue shirt standing in a garden .  \n",
              "4                 Two friends enjoy time spent together .  \n",
              "...                                                   ...  \n",
              "158910   A man in shorts and a Hawaiian shirt leans ov...  \n",
              "158911   A young man hanging over the side of a boat ,...  \n",
              "158912   A man is leaning off of the side of a blue an...  \n",
              "158913   A man riding a small boat in a harbor , with ...  \n",
              "158914   A man on a moored blue and white boat with hi...  \n",
              "\n",
              "[158915 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-edeb47c7-b56b-4689-a6a5-0670d7680126\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_name</th>\n",
              "      <th>comment_number</th>\n",
              "      <th>comment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1000092795.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>Two young guys with shaggy hair look at their...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1000092795.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>Two young , White males are outside near many...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1000092795.jpg</td>\n",
              "      <td>2</td>\n",
              "      <td>Two men in green shirts are standing in a yard .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000092795.jpg</td>\n",
              "      <td>3</td>\n",
              "      <td>A man in a blue shirt standing in a garden .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1000092795.jpg</td>\n",
              "      <td>4</td>\n",
              "      <td>Two friends enjoy time spent together .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158910</th>\n",
              "      <td>998845445.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>A man in shorts and a Hawaiian shirt leans ov...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158911</th>\n",
              "      <td>998845445.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>A young man hanging over the side of a boat ,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158912</th>\n",
              "      <td>998845445.jpg</td>\n",
              "      <td>2</td>\n",
              "      <td>A man is leaning off of the side of a blue an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158913</th>\n",
              "      <td>998845445.jpg</td>\n",
              "      <td>3</td>\n",
              "      <td>A man riding a small boat in a harbor , with ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158914</th>\n",
              "      <td>998845445.jpg</td>\n",
              "      <td>4</td>\n",
              "      <td>A man on a moored blue and white boat with hi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>158915 rows Ã— 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-edeb47c7-b56b-4689-a6a5-0670d7680126')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-edeb47c7-b56b-4689-a6a5-0670d7680126 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-edeb47c7-b56b-4689-a6a5-0670d7680126');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGELIST = df.loc[:, \"image_name\"].unique()\n",
        "captions = {}\n",
        "print(IMAGELIST)\n",
        "print(len(IMAGELIST))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OVCl1whhib8",
        "outputId": "d0d427b0-91de-48f8-d22a-bad5f921f65d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1000092795.jpg' '10002456.jpg' '1000268201.jpg' ... '997876722.jpg'\n",
            " '99804383.jpg' '998845445.jpg']\n",
            "31783\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for (idx, label) in enumerate(IMAGELIST):\n",
        "  #print(idx, label)\n",
        "  captions[idx] = list(df.loc[df[\"image_name\"] == label, \"comment\"].values)"
      ],
      "metadata": {
        "id": "m0DWgr69qKN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "captions[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9i2rmpE5lS0",
        "outputId": "e4dd42b6-f111-40c7-cdbc-3bf82f8c55a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' Two young guys with shaggy hair look at their hands while hanging out in the yard .',\n",
              " ' Two young , White males are outside near many bushes .',\n",
              " ' Two men in green shirts are standing in a yard .',\n",
              " ' A man in a blue shirt standing in a garden .',\n",
              " ' Two friends enjoy time spent together .']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "captions"
      ],
      "metadata": {
        "id": "Fj8YMdb-6uqa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "796c132e-fe36-431d-c7f3-18a598323781"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# captions.keys()"
      ],
      "metadata": {
        "id": "DPKgKYlJ6Kt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary storing all the captions related to particular images\n",
        "with open('captions.pickle', 'wb') as file:\n",
        "    dump(captions, file)\n",
        "\n",
        "del file\n",
        "\n",
        "# storing all the captions according to keys()\n",
        "result = [captions[key] for key in sorted(captions.keys())]\n",
        "with open('captions.pickle', 'wb') as file:\n",
        "    dump(result, file)\n",
        "\n",
        "del file"
      ],
      "metadata": {
        "id": "fIjy2dTRq9h8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csKDI2wbqTjP",
        "outputId": "fd5ddef4-df21-449c-ac83-cd12ce9c4ae0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' Two young guys with shaggy hair look at their hands while hanging out in the yard .',\n",
              " ' Two young , White males are outside near many bushes .',\n",
              " ' Two men in green shirts are standing in a yard .',\n",
              " ' A man in a blue shirt standing in a garden .',\n",
              " ' Two friends enjoy time spent together .']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "03ySenxa6c-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image.py"
      ],
      "metadata": {
        "id": "zCB667Mp7ob-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DIRECTORY : /content/flickr30k_images/flickr30k_images\n",
        "print(IMAGELIST)\n",
        "archive = ZipFile(file=\"/content/flickr-image-dataset.zip\", mode=\"r\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3hMZMkt7p62",
        "outputId": "47a5ef20-3a38-4b07-a48f-c61b076bb03d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1000092795.jpg' '10002456.jpg' '1000268201.jpg' ... '997876722.jpg'\n",
            " '99804383.jpg' '998845445.jpg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ---------------------------------------------------------------------------------------------------------------------\n",
        "### Dividing the whole dataset into 3 parts - images1.pickle, images2.pickle, images3.pickle\n",
        "\n",
        "#### 0-10000 - images1.pickle\n",
        "#### 10000-20000 - images2.pickle\n",
        "#### 20000-Remaining - images3.pickle"
      ],
      "metadata": {
        "id": "DvJOrdJJ_CF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "images = {}\n",
        "start_idx = 0\n",
        "end_idx = 10000\n",
        "\n",
        "\n",
        "for (idx, label) in enumerate(IMAGELIST[start_idx : end_idx]):\n",
        "\n",
        "    img = archive.read(\"flickr30k_images/flickr30k_images/{}\".format(label))\n",
        "    img = BytesIO(img)\n",
        "    img = np.asarray(Image.open(img))\n",
        "    img = resize(img, (300,300))\n",
        "\n",
        "    images[idx+start_idx] = img\n",
        "\n",
        "result = np.array([images[key] for key in sorted(images.keys())])\n",
        "with open('images1.pickle', 'wb') as file:\n",
        "    dump(result, file)\n",
        "\n",
        "del images\n",
        "del file\n"
      ],
      "metadata": {
        "id": "1hpnBGYS773e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = {}\n",
        "start_idx = 10000\n",
        "end_idx = 20000\n",
        "\n",
        "\n",
        "for (idx, label) in enumerate(IMAGELIST[start_idx : end_idx]):\n",
        "\n",
        "    img = archive.read(\"flickr30k_images/flickr30k_images/{}\".format(label))\n",
        "    img = BytesIO(img)\n",
        "    img = np.asarray(Image.open(img))\n",
        "    img = resize(img, (300,300))\n",
        "\n",
        "    images[idx+start_idx] = img\n",
        "\n",
        "result = np.array([images[key] for key in sorted(images.keys())])\n",
        "with open('images2.pickle', 'wb') as file:\n",
        "    dump(result, file)\n",
        "\n",
        "del images\n",
        "del file"
      ],
      "metadata": {
        "id": "PrrG9WP380wy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = {}\n",
        "start_idx = 20000\n",
        "end_idx = None\n",
        "\n",
        "\n",
        "for (idx, label) in enumerate(IMAGELIST[start_idx : ]):\n",
        "\n",
        "    img = archive.read(\"flickr30k_images/flickr30k_images/{}\".format(label))\n",
        "    img = BytesIO(img)\n",
        "    img = np.asarray(Image.open(img))\n",
        "    img = resize(img, (300,300))\n",
        "\n",
        "    images[idx+start_idx] = img\n",
        "\n",
        "result = np.array([images[key] for key in sorted(images.keys())])\n",
        "with open('images3.pickle', 'wb') as file:\n",
        "    dump(result, file)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "del file\n",
        "del images\n",
        "del result"
      ],
      "metadata": {
        "id": "SwWFS2Sy92Rk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EizJGW8Z-54q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sSAVnNeDWk2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Till this point we had done a serialization of the Flickr Dataset\n",
        "\n",
        "Serialization refers to the process of converting a data object (e.g., Python objects, Tensorflow models) into a format that allows us to store or transmit the data and then recreate the object when needed using the reverse process of deserialization"
      ],
      "metadata": {
        "id": "xIqO4xwaWQ4y"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ji-4-7uOWgbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d3WCaJlsWj2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Skip_thoughts.py\n",
        "\n",
        "https://github.com/itsayushthada/Text-To-Image\n",
        "\n",
        "https://github.com/ryankiros/skip-thoughts\n",
        "\n",
        "https://arxiv.org/pdf/1605.05396.pdf"
      ],
      "metadata": {
        "id": "jhJdjBJhahNH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wget"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8G8ev6PkFKzZ",
        "outputId": "3451f730-a00b-4375-ce8f-eb6d77701879"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9674 sha256=ec8f4156e674c41790441f233b3b8c18702bd3889629f2ff79bd98284bb7d84d\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/a8/c3/3cf2c14a1837a4e04bd98631724e81f33f462d86a1d895fae0\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wget\n",
        "wget.download('http://www.cs.toronto.edu/~rkiros/models/dictionary.txt')\n",
        "wget.download('http://www.cs.toronto.edu/~rkiros/models/utable.npy')\n",
        "wget.download('http://www.cs.toronto.edu/~rkiros/models/btable.npy')\n",
        "wget.download('http://www.cs.toronto.edu/~rkiros/models/uni_skip.npz')\n",
        "wget.download('http://www.cs.toronto.edu/~rkiros/models/bi_skip.npz')\n",
        "wget.download('http://www.cs.toronto.edu/~rkiros/models/uni_skip.npz.pkl')\n",
        "wget.download('http://www.cs.toronto.edu/~rkiros/models/bi_skip.npz.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "l6vI-7qjE_O1",
        "outputId": "f730a5ba-8839-4196-ced3-8da5decf1f48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'bi_skip.npz.pkl'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y0D2tl6YGhyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "profile = False\n",
        "\n",
        "#-----------------------------------------------------------------------------#\n",
        "# Specify model and table locations here\n",
        "#-----------------------------------------------------------------------------#\n",
        "path_to_models = '/content/'\n",
        "path_to_tables = '/content'\n",
        "#-----------------------------------------------------------------------------#\n",
        "\n",
        "path_to_umodel = path_to_models + 'uni_skip.npz'\n",
        "path_to_bmodel = path_to_models + 'bi_skip.npz'\n",
        "\n",
        "\n",
        "def load_model():\n",
        "    \"\"\"\n",
        "    Load the model with saved tables\n",
        "    \"\"\"\n",
        "    # Load model options\n",
        "    print('Loading model parameters...')\n",
        "    with open('%s.pkl' % path_to_umodel, 'rb') as f:\n",
        "        uoptions = pkl.load(f)\n",
        "    with open('%s.pkl' % path_to_bmodel, 'rb') as f:\n",
        "        boptions = pkl.load(f)\n",
        "\n",
        "    # Load parameters\n",
        "    uparams = init_params(uoptions)\n",
        "    uparams = load_params(path_to_umodel, uparams)\n",
        "    utparams = init_tparams(uparams)\n",
        "    bparams = init_params_bi(boptions)\n",
        "    bparams = load_params(path_to_bmodel, bparams)\n",
        "    btparams = init_tparams(bparams)\n",
        "\n",
        "    # Extractor functions\n",
        "    print('Compiling encoders...')\n",
        "    embedding, x_mask, ctxw2v = build_encoder(utparams, uoptions)\n",
        "    f_w2v = theano.function([embedding, x_mask], ctxw2v, name='f_w2v')\n",
        "    embedding, x_mask, ctxw2v = build_encoder_bi(btparams, boptions)\n",
        "    f_w2v2 = theano.function([embedding, x_mask], ctxw2v, name='f_w2v2')\n",
        "\n",
        "    # Tables\n",
        "    print('Loading tables...')\n",
        "    utable, btable = load_tables()\n",
        "\n",
        "    # Store everything we need in a dictionary\n",
        "    print('Packing up...')\n",
        "    model = {}\n",
        "    model['uoptions'] = uoptions\n",
        "    model['boptions'] = boptions\n",
        "    model['utable'] = utable\n",
        "    model['btable'] = btable\n",
        "    model['f_w2v'] = f_w2v\n",
        "    model['f_w2v2'] = f_w2v2\n",
        "\n",
        "    return model\n",
        "\n",
        "\t\n",
        "\t\n",
        "\n",
        "def load_tables():\n",
        "    \"\"\"\n",
        "    Load the tables\n",
        "    \"\"\"\n",
        "    words = []\n",
        "    utable = numpy.load(path_to_tables + 'utable.npy', encoding='latin1')\n",
        "    btable = numpy.load(path_to_tables + 'btable.npy', encoding='latin1')\n",
        "    f = open(path_to_tables + 'dictionary.txt', 'rb')\n",
        "    for line in f:\n",
        "        words.append(line.decode('utf-8').strip())\n",
        "    f.close()\n",
        "    utable = OrderedDict(list(zip(words, utable)))\n",
        "    btable = OrderedDict(list(zip(words, btable)))\n",
        "    return utable, btable\n",
        "\n",
        "\t\n",
        "\t\n",
        "class Encoder(object):\n",
        "    \"\"\"\n",
        "    Sentence encoder.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model):\n",
        "      self._model = model\n",
        "\n",
        "    def encode(self, X, use_norm=True, verbose=True, batch_size=128, use_eos=False):\n",
        "      \"\"\"\n",
        "      Encode sentences in the list X. Each entry will return a vector\n",
        "      \"\"\"\n",
        "      return encode(self._model, X, use_norm, verbose, batch_size, use_eos)\t\n",
        "\n",
        "\t  \n",
        "\t  \n",
        "def encode(model, X, use_norm=True, verbose=False, batch_size=128, use_eos=False):\n",
        "    \"\"\"\n",
        "    Encode sentences in the list X. Each entry will return a vector\n",
        "    \"\"\"\n",
        "    # first, do preprocessing\n",
        "    X = preprocess(X)\n",
        "\n",
        "    # word dictionary and init\n",
        "    d = defaultdict(lambda: 0)\n",
        "    for w in list(model['utable'].keys()):\n",
        "        d[w] = 1\n",
        "    ufeatures = numpy.zeros((len(X), model['uoptions']['dim']), dtype='float32')\n",
        "    bfeatures = numpy.zeros((len(X), 2 * model['boptions']['dim']), dtype='float32')\n",
        "\n",
        "    # length dictionary\n",
        "    ds = defaultdict(list)\n",
        "    captions = [s.split() for s in X]\n",
        "    for i, s in enumerate(captions):\n",
        "        ds[len(s)].append(i)\n",
        "\n",
        "    # Get features. This encodes by length, in order to avoid wasting computation\n",
        "    for k in list(ds.keys()):\n",
        "        if verbose:\n",
        "            print(k)\n",
        "        numbatches = len(ds[k]) // batch_size + 1\n",
        "        for minibatch in range(numbatches):\n",
        "            caps = ds[k][minibatch::numbatches]\n",
        "\n",
        "            if use_eos:\n",
        "                uembedding = numpy.zeros((k + 1, len(caps), model['uoptions']['dim_word']), dtype='float32')\n",
        "                bembedding = numpy.zeros((k + 1, len(caps), model['boptions']['dim_word']), dtype='float32')\n",
        "            else:\n",
        "                uembedding = numpy.zeros((k, len(caps), model['uoptions']['dim_word']), dtype='float32')\n",
        "                bembedding = numpy.zeros((k, len(caps), model['boptions']['dim_word']), dtype='float32')\n",
        "            for ind, c in enumerate(caps):\n",
        "                caption = captions[c]\n",
        "                for j in range(len(caption)):\n",
        "                    if d[caption[j]] > 0:\n",
        "                        uembedding[j, ind] = model['utable'][caption[j]]\n",
        "                        bembedding[j, ind] = model['btable'][caption[j]]\n",
        "                    else:\n",
        "                        uembedding[j, ind] = model['utable']['UNK']\n",
        "                        bembedding[j, ind] = model['btable']['UNK']\n",
        "                if use_eos:\n",
        "                    uembedding[-1, ind] = model['utable']['<eos>']\n",
        "                    bembedding[-1, ind] = model['btable']['<eos>']\n",
        "            if use_eos:\n",
        "                uff = model['f_w2v'](uembedding, numpy.ones((len(caption) + 1, len(caps)), dtype='float32'))\n",
        "                bff = model['f_w2v2'](bembedding, numpy.ones((len(caption) + 1, len(caps)), dtype='float32'))\n",
        "            else:\n",
        "                uff = model['f_w2v'](uembedding, numpy.ones((len(caption), len(caps)), dtype='float32'))\n",
        "                bff = model['f_w2v2'](bembedding, numpy.ones((len(caption), len(caps)), dtype='float32'))\n",
        "            if use_norm:\n",
        "                for j in range(len(uff)):\n",
        "                    uff[j] /= norm(uff[j])\n",
        "                    bff[j] /= norm(bff[j])\n",
        "            for ind, c in enumerate(caps):\n",
        "                ufeatures[c] = uff[ind]\n",
        "                bfeatures[c] = bff[ind]\n",
        "\n",
        "    features = numpy.c_[ufeatures, bfeatures]\n",
        "    return features\n",
        "\n",
        "\t\n",
        "\n",
        "def preprocess(text):\n",
        "    \"\"\"\n",
        "    Preprocess text for encoder\n",
        "    \"\"\"\n",
        "    X = []\n",
        "    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "    for t in text:\n",
        "        sents = sent_detector.tokenize(t)\n",
        "        result = ''\n",
        "        for s in sents:\n",
        "            tokens = word_tokenize(s)\n",
        "            result += ' ' + ' '.join(tokens)\n",
        "        X.append(result)\n",
        "    return X\n",
        "\n",
        "\n",
        "\t\n",
        "def nn(model, text, vectors, query, k=5):\n",
        "    \"\"\"\n",
        "    Return the nearest neighbour sentences to query\n",
        "    text: list of sentences\n",
        "    vectors: the corresponding representations for text\n",
        "    query: a string to search\n",
        "    \"\"\"\n",
        "    qf = encode(model, [query])\n",
        "    qf /= norm(qf)\n",
        "    scores = numpy.dot(qf, vectors.T).flatten()\n",
        "    sorted_args = numpy.argsort(scores)[::-1]\n",
        "    sentences = [text[a] for a in sorted_args[:k]]\n",
        "    print(('QUERY: ' + query))\n",
        "    print('NEAREST: ')\n",
        "    for i, s in enumerate(sentences):\n",
        "        print((s, sorted_args[i]))\n",
        "\n",
        "\n",
        "\t\t\n",
        "def word_features(table):\n",
        "    \"\"\"\n",
        "    Extract word features into a normalized matrix\n",
        "    \"\"\"\n",
        "    features = numpy.zeros((len(table), 620), dtype='float32')\n",
        "    keys = list(table.keys())\n",
        "    for i in range(len(table)):\n",
        "        f = table[keys[i]]\n",
        "        features[i] = f / norm(f)\n",
        "    return features\n",
        "\n",
        "\t\n",
        "\n",
        "def nn_words(table, wordvecs, query, k=10):\n",
        "    \"\"\"\n",
        "    Get the nearest neighbour words\n",
        "    \"\"\"\n",
        "    keys = list(table.keys())\n",
        "    qf = table[query]\n",
        "    scores = numpy.dot(qf, wordvecs.T).flatten()\n",
        "    sorted_args = numpy.argsort(scores)[::-1]\n",
        "    words = [keys[a] for a in sorted_args[:k]]\n",
        "    print(('QUERY: ' + query))\n",
        "    print('NEAREST: ')\n",
        "    for i, w in enumerate(words):\n",
        "        print(w)\n",
        "\n",
        "\n",
        "\t\t\n",
        "def _p(pp, name):\n",
        "    \"\"\"\n",
        "    make prefix-appended name\n",
        "    \"\"\"\n",
        "    return '%s_%s' % (pp, name)\n",
        "\n",
        "\n",
        "\t\n",
        "def init_tparams(params):\n",
        "    \"\"\"\n",
        "    initialize Theano shared variables according to the initial parameters\n",
        "    \"\"\"\n",
        "    tparams = OrderedDict()\n",
        "    for kk, pp in list(params.items()):\n",
        "        tparams[kk] = theano.shared(params[kk], name=kk)\n",
        "    return tparams\n",
        "\n",
        "\n",
        "\t\n",
        "def load_params(path, params):\n",
        "    \"\"\"\n",
        "    load parameters\n",
        "    \"\"\"\n",
        "    pp = numpy.load(path)\n",
        "    for kk, vv in list(params.items()):\n",
        "        if kk not in pp:\n",
        "            warnings.warn('%s is not in the archive' % kk)\n",
        "            continue\n",
        "        params[kk] = pp[kk]\n",
        "    return params\n",
        "\n",
        "\n",
        "# layers: 'name': ('parameter initializer', 'feedforward')\n",
        "layers = {'gru': ('param_init_gru', 'gru_layer')}\n",
        "\n",
        "\n",
        "\n",
        "def get_layer(name):\n",
        "    fns = layers[name]\n",
        "    return (eval(fns[0]), eval(fns[1]))\n",
        "\n",
        "\n",
        "\t\n",
        "def init_params(options):\n",
        "    \"\"\"\n",
        "    initialize all parameters needed for the encoder\n",
        "    \"\"\"\n",
        "    params = OrderedDict()\n",
        "\n",
        "    # embedding\n",
        "    params['Wemb'] = norm_weight(options['n_words_src'], options['dim_word'])\n",
        "\n",
        "    # encoder: GRU\n",
        "    params = get_layer(options['encoder'])[0](options, params, prefix='encoder',\n",
        "                                              nin=options['dim_word'], dim=options['dim'])\n",
        "    return params\n",
        "\n",
        "\t\n",
        "\n",
        "def init_params_bi(options):\n",
        "    \"\"\"\n",
        "    initialize all paramters needed for bidirectional encoder\n",
        "    \"\"\"\n",
        "    params = OrderedDict()\n",
        "\n",
        "    # embedding\n",
        "    params['Wemb'] = norm_weight(options['n_words_src'], options['dim_word'])\n",
        "\n",
        "    # encoder: GRU\n",
        "    params = get_layer(options['encoder'])[0](options, params, prefix='encoder',\n",
        "                                              nin=options['dim_word'], dim=options['dim'])\n",
        "    params = get_layer(options['encoder'])[0](options, params, prefix='encoder_r',\n",
        "                                              nin=options['dim_word'], dim=options['dim'])\n",
        "    return params\n",
        "\n",
        "\n",
        "\t\n",
        "def build_encoder(tparams, options):\n",
        "    \"\"\"\n",
        "    build an encoder, given pre-computed word embeddings\n",
        "    \"\"\"\n",
        "    # word embedding (source)\n",
        "    embedding = tensor.tensor3('embedding', dtype='float32')\n",
        "    x_mask = tensor.matrix('x_mask', dtype='float32')\n",
        "\n",
        "    # encoder\n",
        "    proj = get_layer(options['encoder'])[1](tparams, embedding, options,\n",
        "                                            prefix='encoder',\n",
        "                                            mask=x_mask)\n",
        "    print(\"------------------------->\")\n",
        "    print(\"------------------------->\")\n",
        "    print(proj)\n",
        "    ctx = proj[0][-1]\n",
        "\n",
        "    return embedding, x_mask, ctx\n",
        "\n",
        "\n",
        "\t\n",
        "def build_encoder_bi(tparams, options):\n",
        "    \"\"\"\n",
        "    build bidirectional encoder, given pre-computed word embeddings\n",
        "    \"\"\"\n",
        "    # word embedding (source)\n",
        "    embedding = tensor.tensor3('embedding', dtype='float32')\n",
        "    embeddingr = embedding[::-1]\n",
        "    x_mask = tensor.matrix('x_mask', dtype='float32')\n",
        "    xr_mask = x_mask[::-1]\n",
        "\n",
        "    # encoder\n",
        "    proj = get_layer(options['encoder'])[1](tparams, embedding, options,\n",
        "                                            prefix='encoder',\n",
        "                                            mask=x_mask)\n",
        "    projr = get_layer(options['encoder'])[1](tparams, embeddingr, options,\n",
        "                                             prefix='encoder_r',\n",
        "                                             mask=xr_mask)\n",
        "\n",
        "    ctx = tensor.concatenate([proj[0][-1], projr[0][-1]], axis=1)\n",
        "\n",
        "    return embedding, x_mask, ctx\n",
        "\n",
        "\n",
        "# some utilities\n",
        "def ortho_weight(ndim):\n",
        "    W = numpy.random.randn(ndim, ndim)\n",
        "    u, s, v = numpy.linalg.svd(W)\n",
        "    return u.astype('float32')\n",
        "\n",
        "\n",
        "\t\n",
        "def norm_weight(nin, nout=None, scale=0.1, ortho=True):\n",
        "    if nout == None:\n",
        "        nout = nin\n",
        "    if nout == nin and ortho:\n",
        "        W = ortho_weight(nin)\n",
        "    else:\n",
        "        W = numpy.random.uniform(low=-scale, high=scale, size=(nin, nout))\n",
        "    return W.astype('float32')\n",
        "\n",
        "\n",
        "\t\n",
        "def param_init_gru(options, params, prefix='gru', nin=None, dim=None):\n",
        "    \"\"\"\n",
        "    parameter init for GRU\n",
        "    \"\"\"\n",
        "    if nin == None:\n",
        "        nin = options['dim_proj']\n",
        "    if dim == None:\n",
        "        dim = options['dim_proj']\n",
        "    W = numpy.concatenate([norm_weight(nin, dim),\n",
        "                           norm_weight(nin, dim)], axis=1)\n",
        "    params[_p(prefix, 'W')] = W\n",
        "    params[_p(prefix, 'b')] = numpy.zeros((2 * dim,)).astype('float32')\n",
        "    U = numpy.concatenate([ortho_weight(dim),\n",
        "                           ortho_weight(dim)], axis=1)\n",
        "    params[_p(prefix, 'U')] = U\n",
        "\n",
        "    Wx = norm_weight(nin, dim)\n",
        "    params[_p(prefix, 'Wx')] = Wx\n",
        "    Ux = ortho_weight(dim)\n",
        "    params[_p(prefix, 'Ux')] = Ux\n",
        "    params[_p(prefix, 'bx')] = numpy.zeros((dim,)).astype('float32')\n",
        "\n",
        "    return params\n",
        "\n",
        "\n",
        "\t\n",
        "def gru_layer(tparams, state_below, options, prefix='gru', mask=None, **kwargs):\n",
        "    \"\"\"\n",
        "    Forward pass through GRU layer\n",
        "    \"\"\"\n",
        "    nsteps = state_below.shape[0]\n",
        "    if state_below.ndim == 3:\n",
        "        n_samples = state_below.shape[1]\n",
        "    else:\n",
        "        n_samples = 1\n",
        "\n",
        "    dim = tparams[_p(prefix, 'Ux')].shape[1]\n",
        "\n",
        "    if mask == None:\n",
        "        mask = tensor.alloc(1., state_below.shape[0], 1)\n",
        "\n",
        "    def _slice(_x, n, dim):\n",
        "        if _x.ndim == 3:\n",
        "            return _x[:, :, n * dim:(n + 1) * dim]\n",
        "        return _x[:, n * dim:(n + 1) * dim]\n",
        "\n",
        "    state_below_ = tensor.dot(state_below, tparams[_p(prefix, 'W')]) + tparams[_p(prefix, 'b')]\n",
        "    state_belowx = tensor.dot(state_below, tparams[_p(prefix, 'Wx')]) + tparams[_p(prefix, 'bx')]\n",
        "    U = tparams[_p(prefix, 'U')]\n",
        "    Ux = tparams[_p(prefix, 'Ux')]\n",
        "\n",
        "    def _step_slice(m_, x_, xx_, h_, U, Ux):\n",
        "        preact = tensor.dot(h_, U)\n",
        "        preact += x_\n",
        "\n",
        "        r = tensor.nnet.sigmoid(_slice(preact, 0, dim))\n",
        "        u = tensor.nnet.sigmoid(_slice(preact, 1, dim))\n",
        "\n",
        "        preactx = tensor.dot(h_, Ux)\n",
        "        preactx = preactx * r\n",
        "        preactx = preactx + xx_\n",
        "\n",
        "        h = tensor.tanh(preactx)\n",
        "\n",
        "        h = u * h_ + (1. - u) * h\n",
        "        h = m_[:, None] * h + (1. - m_)[:, None] * h_\n",
        "\n",
        "        return h\n",
        "\n",
        "    seqs = [mask, state_below_, state_belowx]\n",
        "    _step = _step_slice\n",
        "\n",
        "    rval, updates = theano.scan(_step,\n",
        "                                sequences=seqs,\n",
        "                                outputs_info=[tensor.alloc(0., n_samples, dim)],\n",
        "                                non_sequences=[tparams[_p(prefix, 'U')],\n",
        "                                               tparams[_p(prefix, 'Ux')]],\n",
        "                                name=_p(prefix, '_layers'),\n",
        "                                n_steps=nsteps,\n",
        "                                profile=profile,\n",
        "                                strict=True)\n",
        "    rval = [rval]\n",
        "\n",
        "\n",
        "    return "
      ],
      "metadata": {
        "id": "2N5oYL6xakcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PnYy6n-sakfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6W1kaVp4akhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data_Loader.py\n",
        "\n",
        "https://github.com/prakashpandey9/Text2Image-PyTorch/blob/master/data_loader.py"
      ],
      "metadata": {
        "id": "XZMnpov4XzCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Text2ImageDataset(Dataset):\n",
        "\n",
        "\tdef __init__(self, img_dir = \"/content/images1.pickle\", cap_dir = \"/content/captions.pickle\"):\n",
        "\t\tself.images = self.load_pickle(img_dir)\n",
        "\t\tself.captions = self.load_pickle(cap_dir)\n",
        "\t\t# self.model = load_model()\n",
        "\n",
        "\tdef load_pickle(self, pickle_path):\n",
        "\t\tobj = 0\n",
        "\t\troot_path = \"\"\n",
        "\t\twith open(root_path + pickle_path, \"rb\") as inputfile:\n",
        "\t\t\tobj = load(inputfile)  \n",
        "\t\treturn obj\n",
        "\n",
        "\tdef encode_captions(self, captions_list):\n",
        "\t\tfun = lambda x: encode(self.model, x) \n",
        "\t\tencoded_list = list(map(fun, captions_list))\n",
        "\t\treturn encoded_list\n",
        "\n",
        "\tdef read_image(self, img_idx):\n",
        "\t\treturn self.images[img_idx]\n",
        "\n",
        "\tdef false_image(self, img_idx):\n",
        "\t\tidx = np.random.randint(0, self.__len__())\n",
        "\t\tif (idx != img_idx):\n",
        "\t\t\treturn self.images[idx]\n",
        "\t\treturn self.false_image(img_idx)\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\treturn self.images.shape[0]\n",
        "\n",
        "\tdef __getitem__(self, idx):\n",
        "\t\tsample = {}\n",
        "\t\tsample[\"true_imgs\"] = torch.FloatTensor(self.read_image(idx))\n",
        "\t\tsample[\"false_imgs\"] = torch.FloatTensor(self.false_image(idx))\n",
        "\t\t# sample[\"true_embds\"] = torch.FloatTensor(self.encode_captions([self.captions[idx]])[0][np.random.randint(0,5)])\n",
        "\t\treturn sample"
      ],
      "metadata": {
        "id": "iV_A4JI1X1rJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a = Text2ImageDataset()"
      ],
      "metadata": {
        "id": "vIQ-R3OKwiwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generator.py"
      ],
      "metadata": {
        "id": "3_q0cbBjyT3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class Generator(nn.Module):\n",
        "\tdef __init__(self, batch_size, img_size, z_dim, text_embed_dim, text_reduced_dim):\n",
        "\t\tsuper(Generator, self).__init__()\n",
        "\n",
        "\t\tself.img_size = img_size\n",
        "\t\tself.z_dim = z_dim\n",
        "\t\tself.text_embed_dim = text_embed_dim\n",
        "\n",
        "\t\tself.concat = nn.Linear(z_dim + text_reduced_dim, 64 * 8 * 4 * 4).cuda()\n",
        "\t\tself.text_reduced_dim = nn.Linear(text_embed_dim, text_reduced_dim).cuda()\n",
        "\t\t\n",
        "\t\t# Defining the generator network architecture\n",
        "\t\tself.d_net = nn.Sequential(\n",
        "\t\t\tnn.ReLU(),\n",
        "\t\t\tnn.ConvTranspose2d(512, 256, 4, 2, 1),\n",
        "\t\t\tnn.BatchNorm2d(256),\n",
        "\t\t\tnn.ReLU(),\n",
        "\t\t\tnn.ConvTranspose2d(256, 128, 4, 2, 1),\n",
        "\t\t\tnn.BatchNorm2d(128),\n",
        "\t\t\tnn.ReLU(),\n",
        "\t\t\tnn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
        "\t\t\tnn.BatchNorm2d(64),\n",
        "\t\t\tnn.ReLU(),\n",
        "\t\t\tnn.ConvTranspose2d(64, 3, 4, 2, 1),\n",
        "\t\t\tnn.Tanh()\n",
        "\t\t).cuda()\n",
        "\n",
        "\tdef forward(self, text, z):\n",
        "\t\t\"\"\" Given a caption embedding and latent variable z(noise), generate an image\n",
        "\t\tArguments\n",
        "\t\t---------\n",
        "\t\ttext : torch.FloatTensor\n",
        "\t\t\tOutput of the skipthought embedding model for the caption\n",
        "\t\t\ttext.size() = (batch_size, text_embed_dim)\n",
        "\t\tz : torch.FloatTensor\n",
        "\t\t\tLatent variable or noise\n",
        "\t\t\tz.size() = (batch_size, z_dim)\n",
        "\t\t--------\n",
        "\t\tReturns\n",
        "\t\t--------\n",
        "\t\toutput : An image of shape (64, 64, 3)\n",
        "\t\t\"\"\"\n",
        "\t\treduced_text = self.text_reduced_dim(text.cuda())  # (batch_size, text_reduced_dim)\n",
        "\t\tconcat = torch.cat((reduced_text, z.cuda()), 1)  # (batch_size, text_reduced_dim + z_dim)\n",
        "\t\tconcat = self.concat(concat)  # (batch_size, 64*8*4*4)\n",
        "\t\tconcat = concat.view(-1, 4, 4, 64 * 8)  # (batch_size, 4, 4, 64*8)\n",
        "\t\t\n",
        "\t\tconcat = concat.permute(0, 3, 1, 2) # (batch_size, 512, 4, 4)\n",
        "\t\td_net_out = self.d_net(concat)  # (batch_size, 3, 64, 64)\n",
        "\t\td_net_out = d_net_out.permute(0, 2, 3, 1) #(batch_size, 64, 64, 3)\n",
        "\t\t\n",
        "\t\toutput = d_net_out / 2. + 0.5   # (batch_size, 64, 64, 3)\n",
        "\n",
        "\t\treturn output"
      ],
      "metadata": {
        "id": "U-vPycqmyWW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c4_0b_Vlyelr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discriminator.py"
      ],
      "metadata": {
        "id": "OXovPQEMyf3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "\tdef __init__(self, batch_size, img_size, text_embed_dim, text_reduced_dim):\n",
        "\t\tsuper(Discriminator, self).__init__()\n",
        "\n",
        "\t\tself.batch_size = batch_size\n",
        "\t\tself.img_size = img_size\n",
        "\t\tself.in_channels = img_size[2]\n",
        "\t\tself.text_embed_dim = text_embed_dim\n",
        "\t\tself.text_reduced_dim = text_reduced_dim\n",
        "\n",
        "\t\t# Defining the discriminator network architecture\n",
        "\t\tself.d_net = nn.Sequential(\n",
        "\t\t\tnn.Conv2d(self.in_channels, 64, 4, 2, 1, bias=False),\n",
        "\t\t\tnn.LeakyReLU(0.2, inplace=True),\n",
        "\t\t\tnn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
        "\t\t\tnn.BatchNorm2d(128),\n",
        "\t\t\tnn.LeakyReLU(0.2, inplace=True),\n",
        "\t\t\tnn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
        "\t\t\tnn.BatchNorm2d(256),\n",
        "\t\t\tnn.LeakyReLU(0.2, inplace=True),\n",
        "\t\t\tnn.Conv2d(256, 512, 4, 2, 1, bias=False),\n",
        "\t\t\tnn.BatchNorm2d(512),\n",
        "\t\t\tnn.LeakyReLU(0.2, inplace=True)).cuda()\n",
        "\n",
        "\t\t# output_dim = (batch_size, 4, 4, 512)\n",
        "\t\t# text.size() = (batch_size, text_embed_dim)\n",
        "\n",
        "\t\t# Defining a linear layer to reduce the dimensionality of caption embedding\n",
        "\t\t# from text_embed_dim to text_reduced_dim\n",
        "\n",
        "\t\tself.cat_net = nn.Sequential(\n",
        "\t\t\tnn.Conv2d(512 + self.text_reduced_dim, 512, 4, 2, 1, bias=False),\n",
        "\t\t\tnn.BatchNorm2d(512),\n",
        "\t\t\tnn.LeakyReLU(0.2, inplace=True)).cuda()\n",
        "\n",
        "\t\tself.text_reduced_dim = nn.Linear(self.text_embed_dim, self.text_reduced_dim).cuda()\n",
        "\t\t\n",
        "\t\tself.linear = nn.Linear(2 * 2 * 512, 1).cuda()\n",
        "\n",
        "\tdef forward(self, image, text):\n",
        "\t\t\"\"\" Given the image and its caption embedding, predict whether the image\n",
        "\t\tis real or fake.\n",
        "\t\tArguments\n",
        "\t\t---------\n",
        "\t\timage : torch.FloatTensor\n",
        "\t\t\timage.size() = (batch_size, 64, 64, 3)\n",
        "\t\ttext : torch.FloatTensor\n",
        "\t\t\tOutput of the skipthought embedding model for the caption\n",
        "\t\t\ttext.size() = (batch_size, text_embed_dim)\n",
        "\t\t--------\n",
        "\t\tReturns\n",
        "\t\t--------\n",
        "\t\toutput : Probability for the image being real/fake\n",
        "\t\tlogit : Final score of the discriminator\n",
        "\t\t\"\"\"\n",
        "\t\timage = image.permute(0, 3, 1, 2) # (batch_size, 3, 64, 64)\n",
        "\t\td_net_out = self.d_net(image)  # (batch_size, 512, 4, 4)\n",
        "\t\td_net_out = d_net_out.permute(0, 2, 3, 1) # (batch_size, 4, 4, 512)\n",
        "\t\t\n",
        "\t\ttext_reduced = self.text_reduced_dim(text)  # (batch_size, text_reduced_dim)\n",
        "\t\ttext_reduced = text_reduced.unsqueeze(1)  # (batch_size, 1, text_reduced_dim)\n",
        "\t\ttext_reduced = text_reduced.unsqueeze(2)  # (batch_size, 1, 1, text_reduced_dim)\n",
        "\t\ttext_reduced = text_reduced.expand(-1, 4, 4, -1)\n",
        "\n",
        "\t\tconcat_out = torch.cat((d_net_out, text_reduced), 3)  # (1, 4, 4, 512+text_reduced_dim)\n",
        "\t\t\n",
        "\t\tlogit = self.cat_net(concat_out.permute(0, 3, 1, 2))\n",
        "\t\tlogit = logit.reshape(-1, logit.size()[1] * logit.size()[2] * logit.size()[3])\n",
        "\t\tlogit = self.linear(logit)\n",
        "\n",
        "\t\toutput = torch.sigmoid(logit)\n",
        "\n",
        "\t\treturn output, logit"
      ],
      "metadata": {
        "id": "FSV8YqDiyhks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "881e_URXyy3g",
        "outputId": "398f9b16-d5a4-46bd-de35-c7af0b38f194"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bi_skip.npz\t dictionary.txt\t\t   images1.pickle  sample_data\n",
            "bi_skip.npz.pkl  drive\t\t\t   images2.pickle  uni_skip.npz\n",
            "btable.npy\t flickr30k_images\t   images3.pickle  uni_skip.npz.pkl\n",
            "captions.pickle  flickr-image-dataset.zip  kaggle.json\t   utable.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# b = GAN_CLS(50,128,10,20,'./log_dir/','./training_checkpoints','./sample_dir','./final_model',10,300,64, 64,64,0.01,0.9,0.99,a)"
      ],
      "metadata": {
        "id": "3z-5Hn5dYMr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train.py"
      ],
      "metadata": {
        "id": "GqfYiDj3y1ps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GAN_CLS(object):\n",
        "\tdef __init__(self, args, data_loader, SUPERVISED=True):\n",
        "\t\t\"\"\"\n",
        "\t\targs : Arguments\n",
        "\t\tdata_loader = An instance of class DataLoader for loading our dataset in batches\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\tself.data_loader = data_loader\n",
        "\t\tself.num_epochs = args.num_epochs\n",
        "\t\tself.batch_size = args.batch_size\n",
        "\n",
        "\t\tself.log_step = args.log_step\n",
        "\t\tself.sample_step = args.sample_step\n",
        "\n",
        "\t\tself.log_dir = args.log_dir\n",
        "\t\tself.checkpoint_dir = args.checkpoint_dir\n",
        "\t\tself.sample_dir = args.sample_dir\n",
        "\t\tself.final_model = args.final_model\n",
        "\t\tself.model_save_step = args.model_save_step \n",
        "\n",
        "\t\t#self.dataset = args.dataset\n",
        "\t\t#self.model_name = args.model_name\n",
        "\n",
        "\t\tself.img_size = args.img_size # 300\n",
        "\t\tself.z_dim = args.z_dim #64\n",
        "\t\tself.text_embed_dim = args.text_embed_dim #64\n",
        "\t\tself.text_reduced_dim = args.text_reduced_dim #64\n",
        "\t\tself.learning_rate = args.learning_rate #0.01\n",
        "\t\tself.beta1 = args.beta1 # 0.9\n",
        "\t\tself.beta2 = args.beta2 # 0.99\n",
        "\t\tself.l1_coeff = args.l1_coeff \n",
        "\t\tself.resume_epoch = args.resume_epoch\n",
        "\t\tself.resume_idx = args.resume_idx\n",
        "\t\tself.SUPERVISED = SUPERVISED\n",
        "\n",
        "\t\t# Logger setting\n",
        "\t\tlog_name = datetime.datetime.now().strftime('%Y-%m-%d')+'.log'\n",
        "\t\tself.logger = logging.getLogger('__name__')\n",
        "\t\tself.logger.setLevel(logging.INFO)\n",
        "\t\tself.formatter = logging.Formatter('%(asctime)s:%(levelname)s:%(message)s')\n",
        "\t\tself.file_handler = logging.FileHandler(os.path.join(self.log_dir, log_name))\n",
        "\t\tself.file_handler.setFormatter(self.formatter)\n",
        "\t\tself.logger.addHandler(self.file_handler)\n",
        "\n",
        "\t\tself.build_model()\n",
        "\n",
        "\tdef smooth_label(self, tensor, offset):\n",
        "\t\treturn tensor + offset\n",
        "\t\t\n",
        "\tdef dump_imgs(images_Array, name):\n",
        "\t\twith open('{}.pickle'.format(name), 'wb') as file:\n",
        "\t\t\tdump(images_Array, file)\n",
        "\t\t\n",
        "\tdef build_model(self):\n",
        "\t\t\"\"\" A function of defining following instances :\n",
        "\t\t-----  Generator\n",
        "\t\t-----  Discriminator\n",
        "\t\t-----  Optimizer for Generator\n",
        "\t\t-----  Optimizer for Discriminator\n",
        "\t\t-----  Defining Loss functions\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\t# ---------------------------------------------------------------------#\n",
        "\t\t#\t\t\t\t\t\t1. Network Initialization\t\t\t\t\t   #\n",
        "\t\t# ---------------------------------------------------------------------#\n",
        "\t\tself.gen = Generator(batch_size=self.batch_size,\n",
        "\t\t\t\t\t\t\t img_size=self.img_size,\n",
        "\t\t\t\t\t\t\t z_dim=self.z_dim,\n",
        "\t\t\t\t\t\t\t text_embed_dim=self.text_embed_dim,\n",
        "\t\t\t\t\t\t\t text_reduced_dim=self.text_reduced_dim)\n",
        "\n",
        "\t\tself.disc = Discriminator(batch_size=self.batch_size,\n",
        "\t\t\t\t\t\t\t\t  img_size=self.img_size,\n",
        "\t\t\t\t\t\t\t\t  text_embed_dim=self.text_embed_dim,\n",
        "\t\t\t\t\t\t\t\t  text_reduced_dim=self.text_reduced_dim)\n",
        "\n",
        "\t\tself.gen_optim = optim.Adam(self.gen.parameters(),\n",
        "\t\t\t\t\t\t\t\t\tlr=self.learning_rate,\n",
        "\t\t\t\t\t\t\t\t\tbetas=(self.beta1, self.beta2))\n",
        "\n",
        "\t\tself.disc_optim = optim.Adam(self.disc.parameters(),\n",
        "\t\t\t\t\t\t\t\t\t lr=self.learning_rate,\n",
        "\t\t\t\t\t\t\t\t\t betas=(self.beta1, self.beta2))\n",
        "\n",
        "\t\tself.cls_gan_optim = optim.Adam(itertools.chain(self.gen.parameters(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tself.disc.parameters()),\n",
        "\t\t\t\t\t\t\t\t\t\tlr=self.learning_rate,\n",
        "\t\t\t\t\t\t\t\t\t\tbetas=(self.beta1, self.beta2))\n",
        "\n",
        "\t\tprint ('-------------  Generator Model Info  ---------------')\n",
        "\t\tself.print_network(self.gen, 'G')\n",
        "\t\tprint ('------------------------------------------------')\n",
        "\n",
        "\t\tprint ('-------------  Discriminator Model Info  ---------------')\n",
        "\t\tself.print_network(self.disc, 'D')\n",
        "\t\tprint ('------------------------------------------------')\n",
        "\n",
        "\t\tself.criterion = nn.BCELoss().cuda()\n",
        "\t\t# self.CE_loss = nn.CrossEntropyLoss().cuda()\n",
        "\t\t# self.MSE_loss = nn.MSELoss().cuda()\n",
        "\t\tself.gen.train()\n",
        "\t\tself.disc.train()\n",
        "\n",
        "\tdef print_network(self, model, name):\n",
        "\t\t\"\"\" A function for printing total number of model parameters \"\"\"\n",
        "\t\tnum_params = 0\n",
        "\t\tfor p in model.parameters():\n",
        "\t\t\tnum_params += p.numel()\n",
        "\n",
        "\t\tprint(model)\n",
        "\t\tprint(name)\n",
        "\t\tprint(\"Total number of parameters: {}\".format(num_params))\n",
        "\n",
        "\tdef load_checkpoints(self, resume_epoch, idx):\n",
        "\t\t\"\"\"Restore the trained generator and discriminator.\"\"\"\n",
        "\t\tprint('Loading the trained models from epoch {} and iteration {}...'.format(resume_epoch, idx))\n",
        "\t\tG_path = os.path.join(self.checkpoint_dir, '{}-{}-G.ckpt'.format(resume_epoch, idx))\n",
        "\t\tD_path = os.path.join(self.checkpoint_dir, '{}-{}-D.ckpt'.format(resume_epoch, idx))\n",
        "\t\tself.gen.load_state_dict(torch.load(G_path, map_location=lambda storage, loc: storage))\n",
        "\t\tself.disc.load_state_dict(torch.load(D_path, map_location=lambda storage, loc: storage))\n",
        "\n",
        "\tdef train_model(self):\n",
        "\n",
        "\t\tdata_loader = self.data_loader\n",
        "\n",
        "\t\tstart_epoch = 0\n",
        "\t\tif self.resume_epoch >= 0:\n",
        "\t\t\tstart_epoch = self.resume_epoch\n",
        "\t\t\tself.load_checkpoints(self.resume_epoch, self.resume_idx)\n",
        "\n",
        "\t\tprint ('---------------  Model Training Started  ---------------')\n",
        "\t\tstart_time = time.time()\n",
        "\n",
        "\t\tfor epoch in range(start_epoch, self.num_epochs):\n",
        "\t\t\tprint(\"Epoch: {}\".format(epoch+1))\n",
        "\t\t\tfor idx, batch in enumerate(data_loader):\n",
        "\t\t\t\tprint(\"Index: {}\".format(idx+1), end = \"\\t\")\n",
        "\t\t\t\ttrue_imgs = batch['true_imgs']\n",
        "\t\t\t\ttrue_embed = batch['true_embds']\n",
        "\t\t\t\tfalse_imgs = batch['false_imgs']\n",
        "\n",
        "\t\t\t\treal_labels = torch.ones(true_imgs.size(0))\n",
        "\t\t\t\tfake_labels = torch.zeros(true_imgs.size(0))\n",
        "\n",
        "\t\t\t\tsmooth_real_labels = torch.FloatTensor(self.smooth_label(real_labels.numpy(), -0.1))\n",
        "\n",
        "\t\t\t\ttrue_imgs = Variable(true_imgs.float()).cuda()\n",
        "\t\t\t\ttrue_embed = Variable(true_embed.float()).cuda()\n",
        "\t\t\t\tfalse_imgs = Variable(false_imgs.float()).cuda()\n",
        "\n",
        "\t\t\t\treal_labels = Variable(real_labels).cuda()\n",
        "\t\t\t\tsmooth_real_labels = Variable(smooth_real_labels).cuda()\n",
        "\t\t\t\tfake_labels = Variable(fake_labels).cuda()\n",
        "\n",
        "\t\t\t\t# ---------------------------------------------------------------#\n",
        "\t\t\t\t# \t\t\t\t\t  2. Training the generator                  #\n",
        "\t\t\t\t# ---------------------------------------------------------------#\n",
        "\t\t\t\tself.gen.zero_grad()\n",
        "\t\t\t\tz = Variable(torch.randn(true_imgs.size(0), self.z_dim)).cuda()\n",
        "\t\t\t\tfake_imgs = self.gen.forward(true_embed, z)\n",
        "\t\t\t\tfake_out, fake_logit = self.disc.forward(fake_imgs, true_embed)\n",
        "\t\t\t\tfake_out = Variable(fake_out.data, requires_grad=True).cuda()\n",
        "\t\t\t\t\n",
        "\t\t\t\ttrue_out, true_logit = self.disc.forward(true_imgs, true_embed)\n",
        "\t\t\t\ttrue_out = Variable(true_out.data, requires_grad=True).cuda()\n",
        "\t\t\t\t\n",
        "\t\t\t\tg_sf = self.criterion(fake_out, real_labels)\n",
        "\t\t\t\t#g_img = self.l1_coeff * nn.L1Loss()(fake_imgs, true_imgs)\n",
        "\t\t\t\tgen_loss = g_sf\n",
        "\n",
        "\t\t\t\tgen_loss.backward()\n",
        "\t\t\t\tself.gen_optim.step()\n",
        "\n",
        "\t\t\t\t# ---------------------------------------------------------------#\n",
        "\t\t\t\t# \t\t\t\t\t3. Training the discriminator\t\t\t\t #\n",
        "\t\t\t\t# ---------------------------------------------------------------#\n",
        "\t\t\t\tself.disc.zero_grad()\n",
        "\t\t\t\tfalse_out, false_logit = self.disc.forward(false_imgs, true_embed)\n",
        "\t\t\t\tfalse_out = Variable(false_out.data, requires_grad=True)\n",
        "\t\t\t\t\n",
        "\t\t\t\tsr = self.criterion(true_out, smooth_real_labels)\n",
        "\t\t\t\tsw = self.criterion(true_out, fake_labels)\n",
        "\t\t\t\tsf = self.criterion(false_out, smooth_real_labels)\n",
        "\t\t\t\t\n",
        "\t\t\t\tdisc_loss =  torch.log(sr) + (torch.log(1-sw) + torch.log(1-sf ))/2 \n",
        "\n",
        "\t\t\t\tdisc_loss.backward()\n",
        "\t\t\t\tself.disc_optim.step()\n",
        "\n",
        "\t\t\t\tself.cls_gan_optim.step()\n",
        "\n",
        "\t\t\t\t# Logging\n",
        "\t\t\t\tloss = {}\n",
        "\t\t\t\tloss['G_loss'] = gen_loss.item()\n",
        "\t\t\t\tloss['D_loss'] = disc_loss.item()\n",
        "\n",
        "\t\t\t\t# ---------------------------------------------------------------#\n",
        "\t\t\t\t# \t\t\t\t\t4. Logging INFO into log_dir\t\t\t\t #\n",
        "\t\t\t\t# ---------------------------------------------------------------#\n",
        "\t\t\t\tlog = \"\"\n",
        "\t\t\t\tif (idx + 1) % self.log_step == 0:\n",
        "\t\t\t\t\tend_time = time.time() - start_time\n",
        "\t\t\t\t\tend_time = datetime.timedelta(seconds=end_time)\n",
        "\t\t\t\t\tlog = \"Elapsed [{}], Epoch [{}/{}], Idx [{}]\".format(end_time, epoch + 1, self.num_epochs, idx)\n",
        "\n",
        "\t\t\t\tfor net, loss_value in loss.items():\n",
        "\t\t\t\t\tlog += \"{}: {:.4f}\".format(net, loss_value)\n",
        "\t\t\t\t\tself.logger.info(log)\n",
        "\t\t\t\t\tprint (log)\n",
        "\t\t\t\t\n",
        "\t\t\t\t\n",
        "\t\t\t\t# ---------------------------------------------------------------#\n",
        "\t\t\t\t# \t\t\t\t\t5. Saving generated images\t\t\t\t\t #\n",
        "\t\t\t\t# ---------------------------------------------------------------#\n",
        "\t\t\t\tif (idx + 1) % self.sample_step == 0:\n",
        "\t\t\t\t\tconcat_imgs = torch.cat((true_imgs, fake_imgs), 0)  # ??????????\n",
        "\t\t\t\t\tconcat_imgs = (concat_imgs + 1) / 2\n",
        "\t\t\t\t\t# out.clamp_(0, 1)\n",
        "\t\t\t\t\t \n",
        "\t\t\t\t\tsave_path = os.path.join(self.sample_dir, '{}-{}-images.jpg'.format(epoch, idx + 1))\n",
        "\t\t\t\t\t# concat_imgs.cpu().detach().numpy()\n",
        "\t\t\t\t\tself.dump_imgs(concat_imgs.cpu().numpy(), save_path)\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\t#save_image(concat_imgs.data.cpu(), self.sample_dir, nrow=1, padding=0)\n",
        "\t\t\t\t\tprint ('Saved real and fake images into {}...'.format(self.sample_dir))\n",
        "\t\t\t\t\n",
        "\t\t\t\t\n",
        "\t\t\t\t# ---------------------------------------------------------------#\n",
        "\t\t\t\t# \t\t\t\t6. Saving the checkpoints & final model\t\t\t #\n",
        "\t\t\t\t# ---------------------------------------------------------------#\n",
        "\t\t\t\tif (idx + 1) % self.model_save_step == 0:\n",
        "\t\t\t\t\tG_path = os.path.join(self.checkpoint_dir, '{}-{}-G.ckpt'.format(epoch, idx + 1))\n",
        "\t\t\t\t\tD_path = os.path.join(self.checkpoint_dir, '{}-{}-D.ckpt'.format(epoch, idx + 1))\n",
        "\t\t\t\t\ttorch.save(self.gen.state_dict(), G_path)\n",
        "\t\t\t\t\ttorch.save(self.disc.state_dict(), D_path)\n",
        "\t\t\t\t\tprint('Saved model checkpoints into {}...\\n'.format(self.checkpoint_dir))\n",
        "\n",
        "\t\tprint ('---------------  Model Training Completed  ---------------')\n",
        "\t\t# Saving final model into final_model directory\n",
        "\t\tG_path = os.path.join(self.final_model, '{}-G.pth'.format('final'))\n",
        "\t\tD_path = os.path.join(self.final_model, '{}-D.pth'.format('final'))\n",
        "\t\ttorch.save(self.gen.state_dict(), G_path)\n",
        "\t\ttorch.save(self.disc.state_dict(), D_path)\n",
        "\t\tprint('Saved final model into {}...'.format(self.final_model))\n",
        "  \n"
      ],
      "metadata": {
        "id": "KBEZ1EA9y3uk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main.py"
      ],
      "metadata": {
        "id": "yzuOH0gSnhAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "#-----------------------------------------------------------------\n",
        "def check_dir(dir_name):\n",
        "    if not os.path.exists(dir_name):\n",
        "        os.makedirs(dir_name)\n",
        "\n",
        "    print ('{} created'.format(dir_name))\n",
        "\n",
        "#-----------------------------------------------------------------\n",
        "def check_args(args):\n",
        "    # Make all directories if they don't exist\n",
        "\n",
        "    # --checkpoint_dir\n",
        "    check_dir(args.checkpoint_dir)\n",
        "\n",
        "    # --sample_dir\n",
        "    check_dir(args.sample_dir)\n",
        "\n",
        "    # --log_dir\n",
        "    check_dir(args.log_dir)\n",
        "\n",
        "    # --final_model dir\n",
        "    check_dir(args.final_model)\n",
        "\n",
        "    # --epoch\n",
        "    assert args.num_epochs > 0, 'Number of epochs must be greater than 0'\n",
        "\n",
        "    # --batch_size\n",
        "    assert args.batch_size > 0, 'Batch size must be greater than zero'\n",
        "\n",
        "    # --z_dim\n",
        "    assert args.z_dim > 0, 'Size of the noise vector must be greater than zero'\n",
        "\n",
        "    return args\n"
      ],
      "metadata": {
        "id": "6QStRYHCzleP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import sys\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    # parser.add_argument_group('Dataset related arguments')\n",
        "    # parser.add_argument('--data_dir', type=str, default=\"Data\",\n",
        "    #                     help='Data Directory')\n",
        "\n",
        "    # parser.add_argument('--dataset', type=str, default=\"flowers\",\n",
        "    #                     help='Dataset to train')\n",
        "\n",
        "    parser.add_argument_group('Model saving path and steps related arguments')\n",
        "    parser.add_argument('--log_step', type=int, default=100,\n",
        "                        help='Save INFO into logger after every x iterations')\n",
        "\n",
        "    parser.add_argument('--sample_step', type=int, default=100,\n",
        "                        help='Save generated image after every x iterations')\n",
        "\n",
        "    parser.add_argument('--checkpoint_dir', type=str, default='checkpoints',\n",
        "                        help='Save model checkpoints after every x iterations')\n",
        "\n",
        "    parser.add_argument('--sample_dir', type=str, default='sample',\n",
        "                        help='Save generated image after every x iterations')\n",
        "\n",
        "    parser.add_argument('--log_dir', type=str, default='logs',\n",
        "                        help='Save INFO into logger after every x iterations')\n",
        "\n",
        "    parser.add_argument('--final_model', type=str, default='final_model',\n",
        "                        help='Save INFO into logger after every x iterations')\n",
        "\n",
        "    parser.add_argument_group('Model training related arguments')\n",
        "    parser.add_argument('--num_epochs', type=int, default=200,\n",
        "                        help='Total number of epochs to train')\n",
        "\n",
        "    parser.add_argument('--batch_size', type=int, default=1,\n",
        "                        help='Batch Size')\n",
        "\n",
        "    parser.add_argument('--img_size', type=int, default=64,\n",
        "                        help='Size of the image')\n",
        "\n",
        "    parser.add_argument('--z_dim', type=int, default=100,\n",
        "                        help='Size of the latent variable')\n",
        "\n",
        "    parser.add_argument('--text_embed_dim', type=int, default=4800,\n",
        "                        help='Size of the embeddding for the captions')\n",
        "\n",
        "    parser.add_argument('--text_reduced_dim', type=int, default=1024,\n",
        "                        help='Reduced dimension of the caption encoding')\n",
        "\n",
        "    parser.add_argument('--learning_rate', type=float, default=0.0002,\n",
        "                        help='Learning Rate')\n",
        "\n",
        "    parser.add_argument('--beta1', type=float, default=0.5,\n",
        "                        help='Hyperparameter of the Adam optimizer')\n",
        "\n",
        "    parser.add_argument('--beta2', type=float, default=0.999,\n",
        "                        help='Hyperparameter of the Adam optimizer')\n",
        "\n",
        "    parser.add_argument('--l1_coeff', type=float, default=50,\n",
        "                        help='Coefficient for the L1 Loss')\n",
        "\n",
        "    parser.add_argument('--resume_epoch', type=int, default=1,\n",
        "                        help='Resume epoch to resume training')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    check_args(args)\n",
        "\n",
        "    dataset = Text2ImageDataset()\n",
        "    data_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "    gan = GAN_CLS(args, data_loader)\n",
        "\n",
        "    gan.build_model()\n",
        "    gan.train_model()\n",
        "\n"
      ],
      "metadata": {
        "id": "L-UUoRRXnvLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "Wskz5mAgn_Yw",
        "outputId": "4da3d709-1d7d-45c7-9633-26b1fbdb45a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: ipykernel_launcher.py [-h] [--log_step LOG_STEP]\n",
            "                             [--sample_step SAMPLE_STEP]\n",
            "                             [--checkpoint_dir CHECKPOINT_DIR]\n",
            "                             [--sample_dir SAMPLE_DIR] [--log_dir LOG_DIR]\n",
            "                             [--final_model FINAL_MODEL]\n",
            "                             [--num_epochs NUM_EPOCHS]\n",
            "                             [--batch_size BATCH_SIZE] [--img_size IMG_SIZE]\n",
            "                             [--z_dim Z_DIM] [--text_embed_dim TEXT_EMBED_DIM]\n",
            "                             [--text_reduced_dim TEXT_REDUCED_DIM]\n",
            "                             [--learning_rate LEARNING_RATE] [--beta1 BETA1]\n",
            "                             [--beta2 BETA2] [--l1_coeff L1_COEFF]\n",
            "                             [--resume_epoch RESUME_EPOCH]\n",
            "ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-5f7f71cd-01f2-4362-9a0f-e878e21d66eb.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py:3334: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = Text2ImageDataset()\n",
        "data_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n"
      ],
      "metadata": {
        "id": "hz-JotonuekO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instead of using the args variables, use the values to be passed into the class of train"
      ],
      "metadata": {
        "id": "Nv91HrNItCQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BXxKs3uzSw64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "59fnoYrkSw92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ayvcAZt5SxAI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}